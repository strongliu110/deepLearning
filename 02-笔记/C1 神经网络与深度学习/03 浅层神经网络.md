# 浅层神经网络

## 3.1 神经网络概览

神经网络的结构与逻辑回归类似，只是神经网络的层数比逻辑回归多一层，多出来的中间那层称为隐藏层或中间层。这样从计算上来说，神经网络的正向传播和反向传播过程只是比逻辑回归多了一次重复的计算。正向传播过程分成两层，第一层是输入层到隐藏层，用上标[1]来表示：

$z^{\left[ 1 \right]}=W^{\left[ 1 \right]}x+b^{\left[ 1 \right]}$

$a^{\left[ 1 \right]}=\sigma \left( z^{\left[ 1 \right]} \right)$

第二层是隐藏层到输出层，用上标[2]来表示：

$z^{\left[ 2 \right]}=W^{\left[ 2 \right]}a^{\left[ 1 \right]}+b^{\left[ 2 \right]}$

$a^{\left[ 2 \right]}=\sigma \left( z^{\left[ 2 \right]} \right)$

在写法上值得注意的是，方括号上标[i]表示当前所处的层数；圆括号上标(i)表示第i个样本。

同样，反向传播过程也分成两层。第一层是输出层到隐藏层，第二层是隐藏层到输入层。

## 3.2 神经网络表示

单隐藏层神经网络就是典型的浅层（shallow）神经网络。

![单隐藏层神经网络](http://img.blog.csdn.net/20170927081305727?)

在写法上，我们通常把输入矩阵X记为$a^{\left[ 0 \right]}$，把隐藏层输出记为$a^{\left[ 1 \right]}$，上标从0开始。用下标表示第几个神经元，注意下标从1开始。例如$a_{1}^{\left[ 1 \right]}$表示隐藏层第1个神经元，$a_{2}^{\left[ 1 \right]}$表示隐藏层第2个神经元，等等。这样，隐藏层有4个神经元就可以将其输出$a^{\left[ 1 \right]}$写成矩阵的形式：
$$
a^{\left[ 1 \right]}=\left[ \begin{array}{c} a_{1}^{\left[ 1 \right]} \\ a_{2}^{\left[ 1 \right]} \\ a_{3}^{\left[ 1 \right]} \\ a_{4}^{\left[ 1 \right]} \end{array} \right]
$$
最后，相应的输出层记为$a^{\left[ 2 \right]}$，即ŷ 。

通常我们只会计算隐藏层输出和输出层的输出，输入层是不用计算的。这也是我们把输入层层数上标记为0的原因（$a^{\left[ 0 \right]}$）。

关于隐藏层对应的权重$w^{\left[ 1 \right]}$和常数项$b^{\left[ 1 \right]}$，$w^{\left[ 1 \right]}$的维度是（4,3）。这里的4对应着隐藏层神经元个数，3对应着输入层x特征向量包含元素个数。常数项$b^{\left[ 1 \right]}$的维度是（4,1）。关于输出层对应的权重$w^{\left[ 2 \right]}$和常数项$b^{\left[ 2 \right]}$，$w^{\left[ 2 \right]}$的维度是（1,4），这里的1对应着输出层神经元个数，4对应着输出层神经元个数。常数项b[2]的维度是（1,1），因为输出只有一个神经元。

总结一下，**第i层的权重$w^{\left[ i \right]}$维度的行等于i层神经元的个数，列等于i-1层神经元的个数；第i层常数项$b^{\left[ i \right]}$维度的行等于i层神经元的个数，列始终为1**。

## 3.3 计算神经网络的输出

![计算神经网络的输出](http://img.blog.csdn.net/20170927081331442?)

**一个神经元可以看作是两步计算的结合: 1. 输入的线性加权$z=w^{T}x+b$； 2. 非线性变换$a=\sigma \left( z \right)$**

对于两层神经网络，从输入层到隐藏层对应一次逻辑回归运算；从隐藏层到输出层对应一次逻辑回归运算。每层计算时，要注意对应的上标和下标，一般我们记上标方括号表示layer，下标表示第几个神经元。例如a[l]i表示第l层的第i个神经元。注意，i从1开始，l从0开始。

为了提高程序运算速度，我们引入向量化和矩阵运算的思想，将上述表达式转换成矩阵运算的形式：

![这里写图片描述](http://img.blog.csdn.net/20170927081521144?)

其中，$W^{\left[ 1 \right]}$的维度是（4,3），$b^{\left[ 1 \right]}$的维度是（4,1），$W^{\left[ 2 \right]}$的维度是（1,4），$b^{\left[ 2 \right]}$的维度是（1,1）。

## 3.4 多个例子中的向量化

在书写标记上用上标(i)表示第i个样本。

对于每个样本i，可以使用for循环来求解其正向输出：

for i = 1 to m: 
​	$z^{\left[ 1 \right]\left( i \right)}=w^{\left[ 1 \right]}x^{\left( i \right)}+b^{\left[ 1 \right]}$

​	$a^{\left[ 1 \right]\left( i \right)}=\sigma \left( z^{\left[ 1 \right]\left( i \right)} \right)$

​	$z^{\left[ 2 \right]\left( i \right)}=w^{\left[ 2 \right]}a^{\left[ 1 \right]\left( i \right)}+b^{\left[ 2 \right]}$

​	$a^{\left[ 2 \right]\left( i \right)}=\sigma \left( z^{\left[ 2 \right]\left( i \right)} \right)$

不使用for循环，利用矩阵运算的思想，输入矩阵X的维度为（$n^{x}$,m）。这样，我们可以把上面的for循环写成矩阵运算的形式：

$Z^{\left[ 1 \right]}=W^{\left[ 1 \right]}X+b^{\left[ 1 \right]}$

$A^{\left[ 1 \right]}=\sigma \left( Z^{\left[ 1 \right]} \right)$

$Z^{\left[ 2 \right]}=W^{\left[ 2 \right]}A^{\left[ 1 \right]}+b^{\left[ 1 \right]}$

$A^{\left[ 2 \right]}=\sigma \left( Z^{\left[ 2 \right]} \right)$

其中，$Z^{\left[ 1 \right]}$的维度是（4,m），4是隐藏层神经元的个数；$A^{\left[ 1 \right]}$的维度与$Z^{\left[ 1 \right]}$相同；$Z^{\left[ 2 \right]}$和$A^{\left[ 2 \right]}$的维度均为（1,m）。对上面这四个矩阵来说，均可以这样来理解：**行表示神经元个数，列表示样本数目m**。

## 3.5 向量化实现的解释

值得注意的是输入矩阵X也可以写成$A^{\left[ 0 \right]}$。

## 3.6 激活函数

![sigmoid](http://img.blog.csdn.net/20170920120759651?)

![tanh](http://img.blog.csdn.net/20170919091551404?)

![relu](http://img.blog.csdn.net/20170919091933651?)

![leaky relu](http://img.blog.csdn.net/20170919092253605?)

如何选择合适的激活函数呢？

首先我们来比较sigmoid函数和tanh函数。对于隐藏层的激活函数，一般来说，tanh函数要比sigmoid函数表现更好一些。因为tanh函数的取值范围在[-1,+1]之间，隐藏层的输出被限定在[-1,+1]之间，可以看成是在0值附近分布，均值为0。这样从隐藏层到输出层，数据起到了归一化（均值为0）的效果。因此，隐藏层的激活函数，tanh比sigmoid更好一些。而对于输出层的激活函数，因为二分类问题的输出取值为{0,+1}，所以一般会选择sigmoid作为激活函数。

观察sigmoid函数和tanh函数，我们发现有这样一个问题，就是当|z|很大的时候，激活函数的斜率（梯度）很小。因此，在这个区域内，梯度下降算法会运行得比较慢。在实际应用中，应尽量避免使z落在这个区域，使|z|尽可能限定在零值附近，从而提高梯度下降算法运算速度。

为了弥补sigmoid函数和tanh函数的这个缺陷，就出现了ReLU激活函数。ReLU激活函数在z大于零时梯度始终为1；在z小于零时梯度始终为0；z等于零时的梯度可以当成1也可以当成0，实际应用中并不影响。对于隐藏层，选择ReLU作为激活函数能够保证z大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度。但当z小于零时，存在梯度为0的缺点，实际应用中，这个缺点影响不是很大。为了弥补这个缺点，出现了Leaky ReLU激活函数，能够保证z小于零是梯度不为0。

最后总结一下，如果是分类问题，输出层的激活函数一般会选择sigmoid函数。但是隐藏层的激活函数通常不会选择sigmoid函数，tanh函数的表现几乎在所用场合都会比sigmoid函数好一些。**实际应用中，通常会会选择使用ReLU或者Leaky ReLU函数，保证梯度下降速度不会太小**。其实，具体选择哪个函数作为激活函数没有一个固定的准确的答案，应该要根据具体实际问题进行验证。

## 3.7 为什么需要非线性激活函数

假设所有的激活函数都是线性的，为了简化计算，我们直接令激活函数g(z)=z，即a=z。那么，浅层神经网络的各层输出为：

$z^{\left[ 1 \right]}=W^{\left[ 1 \right]}x+b^{\left[ 1 \right]}$

$a^{\left[ 1 \right]}=z^{\left[ 1 \right]}$

$z^{\left[ 2 \right]}=W^{\left[ 2 \right]}a^{\left[ 1 \right]}+b^{\left[ 2 \right]}$

$a^{\left[ 2 \right]}=z^{\left[ 2 \right]}$

我们对上式中$a^{\left[ 2 \right]}$进行化简计算：

$a^{\left[ 2 \right]}=W^{\left[ 2 \right]}\left( W^{\left[ 1 \right]}x+b^{\left[ 1 \right]} \right)+b^{\left[ 2 \right]}=W^{\left[ 2 \right]}W^{\left[ 1 \right]}x+W^{\left[ 2 \right]}b^{\left[ 1 \right]}+b^{\left[ 2 \right]}=W'x+b$

经过推导我们发现$a^{\left[ 2 \right]}$仍是输入变量x的线性组合。这表明，使用神经网络与直接使用线性模型的效果并没有什么两样。

值得一提的是，**如果是预测问题而不是分类问题，输出y是连续的情况下，输出层的激活函数可以使用线性函数**。如果输出y恒为正值，则也可以使用ReLU激活函数。

## 3.8 激活函数的导数

- sigmoid函数的导数：

$$
g\left( z \right)=\frac{1}{1+e^{-z}}
$$

$$
g'\left( z \right)=g\left( z \right)\left( 1-g\left( z \right) \right)
$$

推导过程：$g'\left( z \right)=\left( \frac{1}{1+e^{-z}} \right)'=-\frac{e^{-z}\cdot \left( -1 \right)}{\left( 1+e^{-z} \right)^{2}}=\frac{1+e^{-z}-1}{\left( 1+e^{-z} \right)^{2}}=\frac{1}{1+e^{-z}}-\frac{1}{\left( 1+e^{-z} \right)^{2}}=\frac{1}{1+e^{-z}}\left( 1-\frac{1}{1+e^{-z}} \right)=g \left( z \right)\left( 1-g \left( z \right) \right)$

- tanh函数的导数：

$$
g\left( z \right)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}
$$

$$
g'\left( z \right)=1-g^{2}\left( z \right)
$$

推导过程：

$\sinh \left( x \right)=\frac{e^{x}-e^{-x}}{2}$， $\cosh \left( x \right)=\frac{e^{x}+e^{-x}}{2}$， $\tanh \left( x \right)=\frac{\sinh \left( x \right)}{\cosh \left( x \right)}$

$\left( u\cdot x \right)'=u'x+ux'$， $\left( \frac{u}{v} \right)'=\frac{u'v-uv'}{v^{2}}$

$\sinh '\left( x \right)=\cosh \left( x \right)$，$\cosh '\left( x \right)=\sinh \left( x \right)$

$g'\left( z \right)=\left( \frac{\sinh \left( z \right)}{\cosh \left( z \right)} \right)'=\frac{\sinh '\left( z \right)\cdot \cosh \left( z \right)-\sinh \left( z \right)\cdot \cosh '\left( z \right)}{\cosh ^{2}\left( z \right)}=\frac{\cosh \left( z \right)\cdot \cosh \left( z \right)-\sinh \left( z \right)\cdot sih\left( z \right)}{\cosh ^{2}\left( z \right)}=1-\left( \frac{\sinh \left( z \right)}{\cosh \left( z \right)} \right)^{2}=1-g^{2}\left( z \right)$

- ReLU函数的导数：

$$
g\left( z \right)=\max \left( 0,z \right)
$$

$$
g'(z)=
\begin{cases}
0 & {z<0}\\
1 & {z \geq 0}
\end{cases}
$$

- Leaky ReLU函数的导数：

$$
g\left( z \right)=\max \left( 0.01z,z \right)
$$

$$
g'(z)=
\begin{cases}
0.01 & {z<0}\\
1 & {z \geq 0}
\end{cases}
$$

## 3.9 神经网络的梯度下降法

以浅层神经网络为例，包含的参数为$W^{[1]}​$，$b^{[1]}​$ ，$W^{[2]}​$，$b^{[2]}​$。令输入层的特征向量个数$n_{x}=n^{\left[ 0 \right]}​$，隐藏层神经元个数为$n^{\left[ 1 \right]}​$，输出层神经元个数为$n^{\left[ 2 \right]}=1​$。则$W^{[1]}​$的维度为（$n^{[1]}​$,$n^{[0]}​$），$b^{[1]}​$的维度为（$n^{[1]}​$,1），$W^{[2]}​$的维度为（$n^{[2]}​$,$n^{[1]}​$），$b^{[2]}​$的维度为（$n^{[2]}​$,1）。

该神经网络的正向传播过程为：

$Z^{\left[ 1 \right]}=W^{\left[ 1 \right]}X+b^{\left[ 1 \right]}$

$A^{\left[ 1 \right]}=g\left( Z^{\left[ 1 \right]} \right)$

$Z^{\left[ 2 \right]}=W^{\left[ 2 \right]}A^{\left[ 1 \right]}+b^{\left[ 2 \right]}$

$A^{\left[ 2 \right]}=g\left( Z^{\left[ 2 \right]} \right)$

其中，g(⋅)表示激活函数。

类似逻辑回归，反向传播是计算导数（梯度）的过程，这里先列出来成本函数对各个参数的梯度（假设g(⋅)=sigmoid()）：

$dZ^{\left[ 2 \right]}=A^{\left[ 2 \right]}-Y$

$dW^{\left[ 2 \right]}=\frac{1}{m}dZ^{\left[ 2 \right]}A^{\left[ 1 \right]T}$

$db^{\left[ 2 \right]}=\frac{1}{m}np.sum\left( dZ^{\left[ 2 \right]},axis=1,keepdim=True \right)$

$dZ^{\left[ 1 \right]}=W^{\left[ 2 \right]T}dZ^{\left[ 2 \right]}\times g'\left( Z^{\left[ 1 \right]} \right)$

$dW^{\left[ 1 \right]}=\frac{1}{m}dZ^{\left[ 1 \right]}X^{T}$

$db^{\left[ 1 \right]}=\frac{1}{m}np.sum\left( dZ^{\left[ 1 \right]},axis=1,keepdim=True \right)$

其中，没有axis参数表示全部相加，axis＝0表示按列相加，axis＝1表示按照行的方向相加。keepdim=True是为了避免rank 1 array，也可以得到结果后再调用reshape函数。

## 3.10 直观理解反向传播

使用计算图的方式来推导神经网络反向传播过程，其过程如下图所示：

![反向传播](http://img.blog.csdn.net/20170927082308231?)

由于多了一个隐藏层，神经网络的计算图要比逻辑回归的复杂一些，如下图所示。对于单个训练样本，正向过程很容易，反向过程可以根据梯度计算方法逐一推导。**导数的维度与原参数相同。**

$dz^{\left[ 2 \right]}=a^{\left[ 2 \right]}-y$

$dW^{\left[ 2 \right]}=dz^{\left[ 2 \right]}\cdot \frac{\partial z^{\left[ 2 \right]}}{\partial W^{\left[ 2 \right]}}=dz^{\left[ 2 \right]}a^{\left[ 1 \right]T}$

$db^{\left[ 2 \right]}=dz^{\left[ 2 \right]}\cdot \frac{\partial z^{\left[ 2 \right]}}{\partial b^{\left[ 2 \right]}}=dz^{\left[ 2 \right]}\cdot 1=dz^{\left[ 2 \right]}$

$dz^{\left[ 1 \right]}=dz^{\left[ 2 \right]}\cdot \frac{\partial z^{\left[ 2 \right]}}{\partial a^{\left[ 1 \right]}}\cdot \frac{\partial a^{\left[ 1 \right]}}{\partial z^{\left[ 1 \right]}}=W^{\left[ 2 \right]T}dz^{\left[ 2 \right]}\times g^{\left[ 1 \right]'}\left( z^{\left[ 1 \right]} \right)$

$dW^{\left[ 1 \right]}=dz^{\left[ 1 \right]}x^{T}$

$db^{\left[ 1 \right]}=dz^{\left[ 1 \right]}$

![神经网络梯度](http://img.blog.csdn.net/20170927082510979?)

总结一下，浅层神经网络（包含一个隐藏层），m个训练样本的正向传播过程和反向传播过程分别包含了6个表达式，其向量化矩阵形式如下图所示：

![总结](http://img.blog.csdn.net/20170927082924477?)

## 3.11 随机初始化

**随机初始化的必要性: 参数权值W全部初始化为 0, 将引入对称问题 symmetry breaking problem。后果是: 第一隐层的每个神经元进行相同的计算, 输出同一结果, 以致于经过多次迭代, 第一隐层的神经元计算结果仍然相同。**

举个简单的例子，一个浅层神经网络包含两个输入，隐藏层包含两个神经元。如果权重$w^{\left[ 1 \right]}$和$w^{\left[ 2 \right]}$都初始化为零，即：

$w^{\left[ 1 \right]}=\left[ \begin{array}{cc} 0 & 0 \\ 0 & 0 \end{array} \right]$

$w^{\left[ 2 \right]}=\left[ \begin{array}{cc} 0 & 0  \end{array} \right]$

这样使得隐藏层第一个神经元的输出等于第二个神经元的输出，即$a_{1}^{\left[ 1 \right]}=a_{2}^{\left[ 1 \right]}$。经过推导得到$dz_{1}^{\left[ 1 \right]}=dz_{2}^{\left[ 1 \right]}$，以及$dW_{1}^{\left[ 1 \right]}=dW_{2}^{\left[ 1 \right]}$。因此，这样的结果是隐藏层两个神经元对应的权重行向量$W_{1}^{\left[ 1 \right]}$和$W_{2}^{\left[ 1 \right]}$每次迭代更新都会得到完全相同的结果，$W_{1}^{\left[ 1 \right]}$始终等于$W_{2}^{\left[ 1 \right]}$，完全对称。这样隐藏层设置多个神经元就没有任何意义了。值得一提的是，**参数b可以全部初始化为零，并不会影响神经网络训练效果**。

![示例](http://img.blog.csdn.net/20170927082954620?)

我们把这种权重W全部初始化为零带来的问题称为对称性问题(symmetry breaking problem)。解决方法也很简单，就是将W进行随机初始化（b可初始化为零）。python里可以使用如下语句进行W和b的初始化：

```python
W_1 = np.random.randn((2,2))*0.01
b_1 = np.zero((2,1))
W_2 = np.random.randn((1,2))*0.01
b_2 = 0
```

**这里我们将$W_{1}^{\left[ 1 \right]}$和$W_{2}^{\left[ 1 \right]}$乘以0.01的目的是尽量使得权重W初始化比较小的值**。之所以让W比较小，是因为如果使用sigmoid函数或者tanh函数作为激活函数的话，W比较小，得到的|z|也比较小（靠近零点），而零点区域的梯度比较大，这样能大大提高梯度下降算法的更新速度，尽快找到全局最优解。如果W较大，得到的|z|也比较大，附近曲线平缓，梯度较小，训练过程会慢很多。

当然，如果激活函数是ReLU或者Leaky ReLU函数，则不需要考虑这个问题。但是，如果输出层是sigmoid函数，则对应的权重W最好初始化到比较小的值。

