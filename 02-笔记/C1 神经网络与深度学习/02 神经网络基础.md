# 神经网络基础

## 2.1 二分分类

我们知道逻辑回归模型一般用来解决二分类（Binary Classification）问题。二分类就是输出y只有{0,1}两个离散值（也有{-1,1}）的情况。

吴恩达的矩阵表示法: **用列向量表示一个样本**, 因此 `X.shape==(n_x, m)`, n_x 是特征数, m 是样本大小。之所以采用(n_x, m)而不是(m, n_x)的原因是为了之后矩阵运算的方便。而所有训练样本的输出Y也组成了一维的行向量，写成矩阵的形式后，它的维度就是(1, m)。

## 2.2 logistic 回归

逻辑回归中， $\hat{y}=P\left( y=1|x \right)$表示预测值为1的概率，取值范围在[0,1]之间。这是其与二分分类模型不同的地方。使用线性模型，引入参数w和b。权重w的维度是($n^{x}$,1)，b是一个常数项。这样，逻辑回归的线性预测输出可以写成： $\hat{y}=w^{T}x+b$。值得注意的是，很多其它机器学习资料中，可能把常数b当做$w^{0}$处理，并引入$x^{0}=1$。这样从维度上来看，x和w都会增加一维。但在本课程中，为了简化计算和便于理解，Andrew建议还是使用上式这种形式**将w和b分开比较好**。

上式的线性输出区间为整个实数范围，而逻辑回归要求输出范围在[0,1]之间，所以还需要对上式的线性函数输出进行处理。方法是引入Sigmoid函数，让输出限定在[0,1]之间。这样，逻辑回归的预测输出就可以完整写成：

$\hat{y}=\mbox{Si}gmoid\left( w^{T}x+b \right)=\sigma \left( w^{T}x+b \right)$

Sigmoid函数是一种非线性的S型函数，输出被限定在[0,1]之间，通常被用在神经网络中当作激活函数（Activation function）使用。Sigmoid函数的表达式如下所示：$sigmoid\left( z \right)=\frac{1}{1+e^{-z}}$

从Sigmoid函数曲线可以看出，当z值很大时，函数值趋向于1；当z值很小时，函数值趋向于0。且当z=0时，函数值为0.5。还有一点值得注意的是，**Sigmoid函数的一阶导数可以用其自身表示**：$\sigma '\left( z \right)=\sigma \left( z \right)\left( 1-\sigma \left( z \right) \right)$。这样，通过Sigmoid函数，就能够将逻辑回归的输出限定在[0,1]之间了。

## 2.3 logistic 回归损失函数

**成本函数(cost function)**是所有样本的误差总和；**损失函数(loss function)**是单个样本的估计值与真实值的误差。

逻辑回归中，w和b都是未知参数，需要反复训练优化得到。因此，我们需要定义一个成本函数，包含了参数w和b。通过优化成本函数，当成本函数取值最小时，得到对应的w和b。

对于m个训练样本，我们通常使用上标来表示对应的样本。例如($x^{\left( i \right)}$,$y^{\left( i \right)}$)表示第i个样本。

由于**平方错误函数$L\left(\hat{y} ,y \right)=\frac{1}{2}\left(\hat{y} −y \right)^{2}$是非凸的**，因此对于逻辑回归，我们一般不使用平方错误作为损失函数。**非凸函数在使用梯度下降算法时，容易得到局部最小值**（即局部最优化）。而我们最优化的目标是计算得到全局最优化，因此我们应该选择凸函数。

损失函数的原则和目的就是要衡量预测输出$\hat{y}$与真实样本输出y的接近程度。平方错误其实也可以，只是它是非凸的，不利于使用梯度下降算法来进行全局优化。因此，我们可以构建另外一种损失函数，且是凸函数的，如下所示：$L\left( \hat{y},y \right)=-\left( y\log \hat{y}+\left( 1-y \right)\log \left( 1-\hat{y} \right) \right)$

- 当y=1时， $L\left( \hat{y},y \right)=-\log \hat{y}$。如果$\hat{y}$越接近1，$L\left( \hat{y},y \right) \approx 0$ ，表示预测效果越好；如果$\hat{y}$越接近0, $L\left( \hat{y},y \right) \approx +\infty $，表示预测效果越差。


- 当y=0时，$L\left( \hat{y},y \right)=-\log \left( 1-\hat{y} \right)$。如果$\hat{y}$越接近0，$L\left( \hat{y},y \right) \approx 0$ ，表示预测效果越好；如果$\hat{y}$越接近1，$L\left( \hat{y},y \right) \approx +\infty $，表示预测效果越差。

成本函数是m个样本的损失函数的平均值，反映了m个样本的预测输出$\hat{y}$与真实样本输出y的平均接近程度。成本函数可表示为：$J\left( w,b \right)=\frac{1}{m}\sum_{i=1}^{m}{L\left( \hat{y} \left( i \right),y\left( i \right) \right)}=-\frac{1}{m}\sum_{i=1}^{m}{\left[ y\left( i \right)\log \hat{y} \left( i \right)+\left( 1−y\left( i \right) \right)\log \; \left( 1−\hat{y} \left( i \right) \right) \right]}$

## 2.4 梯度下降法

由于成本函数J(w,b)是凸函数（convex function），梯度下降算法是先随机选择一组参数w和b值，然后每次迭代的过程中分别沿着w和b的梯度（偏导数）的反方向前进一小步，不断修正w和b。每次迭代更新w和b后，都能让J(w,b)更接近全局最小值。梯度下降的过程如下图所示。

![梯度下降法](http://img.blog.csdn.net/20170926083044469?)

**梯度下降算法每次迭代更新，w和b的修正表达式为：**

$w\; :=\; w\; -\; \alpha \frac{\partial J\left( w,b \right)}{\partial w}$

$b\; :=\; b\; -\; \alpha \frac{\partial J\left( w,b \right)}{\partial b}$

上式中，$\alpha $是学习因子（learning rate），表示梯度下降的步进长度。

梯度下降算法能够保证每次迭代w和b都能向着J(w,b)全局最小化的方向进行。**其数学原理主要是运用泰勒一阶展开来证明的**。

## 2.7 计算图 & 2.8 计算图的导数计算

整个神经网络的训练过程实际上包含了两个过程：**正向传播（Forward Propagation）和反向传播（Back Propagation）**。正向传播是从输入到输出，由神经网络计算得到预测输出的过程；反向传播是从输出到输入，对参数w和b计算梯度的过程。

一个辅助工具: 计算图 Computation Graph (和 Tensorflow 中的概念差不多) 是一个有向图，边 edge 定义了数据流动的方向，节点 node表示运算操作 operation，流通的是数据。

![/img/2017-08-21-andrew_ng_dl_course1_note/computation_graph.png](http://kissg.me/img/2017-08-21-andrew_ng_dl_course1_note/computation_graph.png)

上图中蓝线表示正向传播, 红线表示反向传播。

## 2.9 logistic 回归中的梯度下降法

对单个样本而言，逻辑回归的损失函数表达式如下：

$z=w^{T}x+b$

$\hat{y}=a=\sigma \left( z \right)$

$L\left( a,y \right)=-\left( y\log a+\left( 1-y \right)\log \left( 1-a \right) \right)$

首先，该逻辑回归的正向传播过程非常简单。根据上述公式，例如输入样本x有两个特征($x_{1}$,$x_{2}$)，相应的权重w维度也是2，即($w_{1}$,$w_{2}$)。则$z=w_{1}x_{1}+w_{2}x_{2}+b$，最后的损失函数如下所示：

![这里写图片描述](http://img.blog.csdn.net/20170926083257736?)

然后，计算该逻辑回归的反向传播过程，即由损失函数计算参数w和b的偏导数。推导过程如下：

$da=\frac{\partial L}{\partial a}=-\frac{y}{a}+\frac{1-y}{1-a}$

$dz=\frac{\partial L}{\partial z}=\frac{\partial L}{\partial a}\cdot \frac{\partial a}{\partial z}=\left( -\frac{y}{a}+\frac{1-y}{1-a} \right)\cdot a\left( 1-a \right)=a-y$

知道了dz之后，就可以直接对w1，w2和b进行求导了。

$dw_{1}=\frac{\partial L}{\partial w_{1}}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w_{1}}=x_{1}\cdot dz=x_{1}\left( a-y \right)$

$dw_{2}=\frac{\partial L}{\partial w_{2}}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial w_{2}}=x_{2}\cdot dz=x_{2}\left( a-y \right)$

$db=\frac{\partial L}{\partial b}=\frac{\partial L}{\partial z}\cdot \frac{\partial z}{\partial b}=1\cdot dz=a-y$

则梯度下降算法可表示为：

$w_{1}:=w_{1}-\alpha dw_{1}$

$w_{2}:=w_{2}-\alpha dw_{2}$

$b:=b-\alpha db$

![这里写图片描述](http://img.blog.csdn.net/20170926083336211?)

## 2.10 m个样本的梯度下降

如果有m个样本，其成本函数表达式如下：

$z^{\left( i \right)}=w^{T}x^{\left( i \right)}+b$

$\hat{y}^{\left( i \right)}=a^{\left( i \right)}=\sigma \left( z^{\left( i \right)} \right)$

$J\left( w,b \right)=\frac{1}{m}\sum_{i=1}^{m}{L\left( \hat{y}\left( i \right),y\left( i \right) \right)}=-\frac{1}{m}\sum_{i=1}^{m}{\left[ y\left( i \right)\log \hat{y} \left( i \right)+\left( 1−y\left( i \right) \right)\log \; \left( 1−\hat{y} \left( i \right) \right) \right]}$

成本函数关于w和b的偏导数可以写成和平均的形式：

$dw_{1}=\frac{1}{m}\sum_{i=1}^{m}{x_{1}^{\left( i \right)}\left( a^{\left( i \right)}-y^{\left( i \right)} \right)}$

$dw_{2}=\frac{1}{m}\sum_{i=1}^{m}{x_{2}^{\left( i \right)}\left( a^{\left( i \right)}-y^{\left( i \right)} \right)}$

$db=\frac{1}{m}\sum_{i=1}^{m}{\left( a^{\left( i \right)}-y^{\left( i \right)} \right)}$

这样，每次迭代中w和b的梯度有m个训练样本计算平均值得到。这样，每次迭代中w和b的梯度有m个训练样本计算平均值得到。其算法流程图如下所示：

```python
J=0; dw1=0; dw2=0; db=0;
for i = 1 to m
    z(i) = wx(i)+b;
    a(i) = sigmoid(z(i));
    J += -[y(i)log(a(i))+(1-y(i))log(1-a(i))];
    dz(i) = a(i)-y(i);
    dw1 += x1(i)dz(i);
    dw2 += x2(i)dz(i);
    db += dz(i);
J /= m;
dw1 /= m;
dw2 /= m;
db /= m;
```

经过每次迭代后，根据梯度下降算法，w和b都进行更新：

$w_{1}:=w_{1}-\alpha dw_{1}$

$w_{2}:=w_{2}-\alpha dw_{2}$

$b:=b-\alpha db$

这样经过n次迭代后，整个梯度下降算法就完成了。

在深度学习中，样本数量m通常很大，使用for循环会让神经网络程序运行得很慢。所以，我们应该尽量避免使用for循环操作，而使用矩阵运算，能够大大提高程序运行速度。

## 2.11 向量化

向量化（Vectorization）就是**利用矩阵运算来代替循环**，从而充分利用SIMD, 单指令多数据的优势，提高计算效率。

为了加快深度学习神经网络运算速度，可以使用比CPU运算能力更强大的GPU。事实上，GPU和CPU都有并行指令（parallelization instructions），称为Single Instruction Multiple Data（SIMD）。SIMD是单指令多数据流，能够复制多个操作数，并把它们打包在大型寄存器的一组指令集。SIMD能够大大提高程序运行速度，例如python的numpy库中的内建函数（built-in function）就是使用了SIMD指令。相比而言，GPU的SIMD要比CPU更强大一些。

## 2.12 向量化的更多例子

在python的numpy库中，我们通常使用np.dot()函数来进行矩阵运算。

我们将向量化的思想使用在逻辑回归算法上，尽可能减少for循环，而只使用矩阵运算。值得注意的是，**算法最顶层的迭代训练的for循环是不能替换的**。而每次迭代过程对J，dw，b的计算是可以直接使用矩阵运算。

## 2.13 向量化 logistic 回归

整个训练样本构成的输入矩阵X的维度是（$n_{x}$，m），权重矩阵w的维度是（$n_{x}$，1），b是一个常数值，而整个训练样本构成的输出矩阵Y的维度为（1，m）。利用向量化的思想，所有m个样本的线性输出Z可以用矩阵表示：$Z=w^{T}X+b$

在python的numpy库中可以表示为：

```python
Z = np.dot(w.T,X) + b
A = sigmoid(Z)
```

其中，w.T表示w的转置。

这样，我们就能够使用向量化矩阵运算代替for循环，对所有m个样本同时运算，大大提高了运算速度。

## 2.14 向量化 logistic 回归的梯度输出

再来看逻辑回归中的梯度下降算法如何转化为向量化的矩阵形式。对于所有m个样本，dZ的维度是（1，m），可表示为：$dZ=A-Y$

db可表示为：$db=\frac{1}{m}\sum_{i=1}^{m}{dz^{\left( i \right)}}$

对应的程序为：

```python
db = 1/m * np.sum(dZ)
```

dw可表示为：$dw=\frac{1}{m}X\cdot dZ^{T}$

对应的程序为：

```python
dw = 1/m * np.dot(X, dZ.T)
```

这样，我们把整个逻辑回归中的for循环尽可能用矩阵运算代替，对于单次迭代，梯度下降算法流程如下所示：

```python
Z = np.dot(w.T, X) + b
A = sigmoid(Z)
dZ = A - Y
dw = 1/m * np.dot(X, dZ.T)
db = 1/m * np.sum(dZ)

w = w - alpha * dw
b = b - alpha * db
```

其中，alpha是学习因子，决定w和b的更新速度。上述代码只是对单次训练更新而言的，外层还需要一个for循环，表示迭代次数。

## 2.15 Python 中的广播

(m, n) 维的矩阵与 (1, n) 或 (m, 1) 维的矩阵进行四则运算时，后者将自动进行横向或纵向复制，得到 (m, n) 维的矩阵，然后计算。**至少保证有一个维度是相同的**。

为了保证矩阵运算正确，可以使用reshape()函数来对矩阵设定所需的维度。

## 2.16 关于 python/numpy 向量的说明

 `np.random.randn(n).shape # (n,)` 得到一个秩为 n 的数组, 既不是行向量也不是列向量（**避免rank 1 array**）。

 `np.random.randn((n, 1)).shape # (n, 1)` 得到一个 (n, 1) 维的的列向量。

`assert(a.shape == (5,1))` 可以使用assert语句对向量或数组的维度进行判断。

`a.reshape((5,1))` 可以使用reshape函数对数组设定所需的维度。

## 2.17 Jupyter/iPython notebook 笔记本的快速指南

http://jupyter.org

## 2.18 logistic 损失函数的解释

首先，预测输出$\hat{y}$的表达式可以写成：$\hat{y}=\sigma \left( w^{T}x+b \right)$。其中，$\sigma \left( z \right)=\frac{1}{1+e^{-z}}$。

$\hat{y}$可以看成是预测输出为正类（+1）的概率：$\hat{y}=P\left( y=1|x \right)$。

当y=1时：$p\left( y|x \right)=\hat{y}$；当y=0时：$p\left( y|x \right)=1-\hat{y}$

我们把上面两个式子整合到一个式子中，得到：$P\left( y|x \right)=\hat{y} ^{y}\left( 1−\hat{y}  \right)^{\left( 1−y \right)}$

由于log函数的单调性，可以对上式$p\left( y|x \right)$进行log处理：$\log P\left( y|x \right)=y\log \hat{y}+\left( 1-y \right)\log \left( 1−\hat{y}  \right)$

我们希望上述概率越大越好，对上式加上负号，则转化成了单个样本的Loss function，越小越好，也就得到了我们之前介绍的逻辑回归的损失函数：$L\left( \hat{y},y \right)=-\left( y\log \hat{y}+\left( 1-y \right)\log \left( 1-\hat{y} \right) \right)$

如果对于所有m个训练样本，假设样本之间是独立同分布（iid）的，我们希望总的概率越大越好：

$\max \prod_{i=1}^{m}{P\left( y^{\left( i \right)}|x^{\left( i \right)} \right)}$

同样引入log函数，加上负号，将上式转化为成本函数：

$J\left( w,b \right)=\frac{1}{m}\sum_{i=1}^{m}{L\left( \hat{y}^{\left( i \right)},y^{\left( i \right)} \right)=-\frac{1}{m}\sum_{i=1}^{m}{\left[ y^{\left( i \right)}\log \hat{y}^{\left( i \right)}+\left( 1-y^{\left( i \right)} \right)\log \left( 1-\hat{y}^{\left( i \right)} \right) \right]}}$

上式中，$\frac{1}{m}$表示对所有m个样本的成本函数求平均，是缩放因子。

