# 深度卷积模型：案例研究

[TOC]

## 2.1 为什么要进行实例探究

通过研究别人构建有效组件的案例，来学习如何把这些基础构建组合起来，形成有效的卷积神经网络。

实际上，在计算机视觉任务中表现良好的神经网络框架，往往也适用于其他任务。

典型的CNN模型包括：LeNet-5，AlexNet，VGG。

除了这些性能良好的CNN模型之外，我们还会介绍残差网络（Residual Network）。其特点是可以构建很深很深的神经网络（目前最深的好像有152层）。

## 2.2 经典网络

**LeNet-5模型**是Yann LeCun教授于1998年提出来的，它是第一个成功应用于**数字识别问题的卷积神经网络**。在MNIST数据中，它的准确率达到大约99.2%。典型的LeNet-5结构包含CONV layer，POOL layer和FC layer，顺序一般是CONV layer->POOL layer->CONV layer->POOL layer->FC layer->FC layer->OUTPUT layer，即$\hat{y}$ 。下图所示的是一个数字识别的LeNet-5的模型结构：

![LeNet-5](http://img.blog.csdn.net/20171211150034018?)

该LeNet模型总共包含了大约6万个参数。值得一提的是，当时Yann LeCun提出的LeNet-5模型池化层使用的是average pool，而且各层激活函数一般是Sigmoid和tanh。现在，我们可以根据需要，做出改进，使用max pool和激活函数ReLU。

**AlexNet模型**是由Alex Krizhevsky、Ilya Sutskever和Geoffrey Hinton共同提出的，其结构如下所示：

![AlexNet](http://img.blog.csdn.net/20171211155720094?)

AlexNet模型与LeNet-5模型类似，只是要复杂一些，总共包含了大约6千万个参数。同样可以根据实际情况使用激活函数ReLU。原作者还提到了一种优化技巧，叫做**局部响应归一化层（Local Response Normalization，简称LRN）**。 而在实际应用中，LRN的效果并不突出。但正是通过这篇paper（2012年赢得图像分类比赛ILSVRC冠军），计算机视觉群体开始重视深度学习，并确信深度学习可以应用于计算机视觉领域。

**VGG-16模型**更加复杂一些，一般情况下，其CONV layer和POOL layer设置如下：

- CONV = 3x3 filters, s = 1, same
- MAX-POOL = 2x2, s = 2

VGG-16结构如下所示：

![VGG-16](http://img.blog.csdn.net/20171211175203203?)

VGG-16的参数多达1亿3千万。然而它结构并不复杂，网络结构很规整，同时卷积层过滤器数量变化存在规律（2倍直到512）。它的主要缺点是是需要训练的特征数量非常大。

paper揭示了，随着网络的加深，图像的高度和宽度都在以一定的规律不断缩小，每次池化后刚好缩小一半，而信道数量在不断增加，而且刚好也是在每组卷积操作后增加一倍。也就是说，图像缩小的比例和信道增加的比例是有规律的。从这个角度来看，这篇paper很吸引人。

## 2.3 残差网络

我们知道，如果神经网络层数越多，网络越深，源于梯度消失和梯度爆炸的影响，整个模型难以训练成功。解决的方法之一是：人为地让神经网络某些层跳过下一层神经元的连接，隔层相连，弱化每层之间的强联系。这种神经网络被称为**残差网络（Residual Networks，简称ResNet）**。

Residual Networks由许多隔层相连的神经元子模块组成，我们称之为Residual block。单个Residual block的结构如下图所示：

![残差单元](http://img.blog.csdn.net/20171211204756960?)

上图中红色部分就是skip connection，直接建立$a^{\left[ l \right]}$与$a^{\left[ l+2 \right]}$之间的隔层联系。相应的表达式如下：
$$
z^{[l+1]}=W^{[l+1]}a^{[l]}+b^{[l+1]}
$$

$$
a^{[l+1]}=g(z^{[l+1]})
$$

$$
z^{[l+2]}=W^{[l+2]}a^{[l+1]}+b^{[l+2]}
$$

$$
a^{[l+2]}=g(z^{[l+2]}+a^{[l]})
$$

$a^{\left[ l \right]}$直接隔层与下一层的线性输出相连，与$z^{\left[ l+2 \right]}$共同通过激活函数（ReLU）输出$a^{\left[ l+2 \right]}$。

该模型由Kaiming He, Xiangyu Zhang, Shaoqing Ren和Jian Sun共同提出。由多个Residual block组成的神经网络就是Residual Network。实验表明，这种模型结构对于训练非常深的神经网络，效果很好。另外，为了便于区分，我们把非Residual Networks称为Plain Network。

![残差网络](http://img.blog.csdn.net/20171211211417392?)

Residual Network的结构如上图所示。

与Plain Network相比，Residual Network能够训练更深层的神经网络，有效避免发生发生梯度消失和梯度爆炸。从下面两张图的对比中可以看出，随着神经网络层数增加，Plain Network实际性能会变差，training error甚至会变大。然而，Residual Network的训练效果却很好，training error一直呈下降趋势。

![残差网络与简朴网络比较](http://img.blog.csdn.net/20171211213835572?)

## 2.4 残差网络为什么有用？

下面用个例子来解释为什么ResNets能够训练更深层的神经网络。

![残差网络](http://img.blog.csdn.net/20171211215418919?)

如上图所示，输入x经过很多层神经网络后输出$a^{[l]}$，a[l]经过一个Residual block输出$a^{[l+2]}$。$$a^{[l+2]}$$的表达式为：

$a^{\left[ l+2 \right]}=g\left( z^{\left[ l+2 \right]}+a^{\left[ l \right]} \right)=g\left( W^{\left[ l+2 \right]}a^{\left[ l+1 \right]}+b^{\left[ l+2 \right]}+a^{\left[ l \right]} \right)$

输入x经过Big NN后，若$W^{\left[ l+2 \right]} \approx 0$，$b^{\left[ l+2 \right]} \approx0$ ，则有：

可以看出，即使发生了梯度消失，$W^{\left[ l+2 \right]} \approx 0$，$b^{\left[ l+2 \right]} \approx0$，也能直接建立$a^{\left[ l+2 \right]}$与$a^{\left[ l \right]}$的线性关系

，且$a^{\left[ l+2 \right]}=a^{\left[ l \right]}$，这其实就是identity function。$a^{\left[ l \right]}$直接连到$a^{\left[ l+2 \right]}$，从效果来说，相当于直接忽略了$a^{\left[ l \right]}$$之后的这两层神经层。这样，看似很深的神经网络，其实由于许多Residual blocks的存在，弱化削减了某些神经层之间的联系，实现隔层线性传递，而不是一味追求非线性关系，模型本身也就能“容忍”更深层的神经网络了。而且从性能上来说，这两层额外的Residual blocks也不会降低Big NN的性能。

当然，如果Residual blocks确实能训练得到非线性关系，那么也会忽略short cut，跟Plain Network起到同样的效果。

有一点需要注意的是，如果Residual blocks中$a^{[l]}$和$a^{[l+2]}$的维度不同，通常可以引入矩阵$W_{s}$，与$a^{[l]}$相乘，使得$W_{s}\times a^{\left[ l \right]}$的维度$a^{[l+2]}$与一致。参数矩阵$W_{s}$有来两种方法得到：一种是将$W_{s}$作为学习参数，通过模型训练得到；另一种是固定$W_{s}$值（类似单位矩阵），不需要训练，$W_{s}$与$a^{\left[ l \right]}$的乘积仅仅使得$a^{\left[ l \right]}$截断或者补零。这两种方法都可行。

下图所示的是CNN中ResNets的结构：

![ResNets结构](http://img.blog.csdn.net/20171212142205247?)

ResNets同类型层之间，例如CONV layers，大多使用相同类型，保持维度相同。如果是不同类型层之间的连接，例如CONV layer与POOL layer之间，如果维度不同，则引入矩阵$W_{s}$。

## 2.5 网络中的网络（ 1*1卷积）

Min Lin, Qiang Chen等人提出了一种新的CNN结构，**即1x1 Convolutions，也称Networks in Networks**。这种结构的特点是滤波器算子filter的维度为1x1。对于单个filter，1x1的维度，**意味着卷积操作等同于乘积操作**。

![1*1网络](http://img.blog.csdn.net/20171212144305558?)

那么，对于多个filters，1x1 Convolutions的作用实际上类似全连接层的神经网络结构。效果等同于Plain Network中a[l]到a[l+1]的过程。这点还是比较好理解的。

![1*1](http://img.blog.csdn.net/20171212144647936?)

**1x1 Convolutions可以用来缩减输入图片的通道数目**。方法如下图所示：

![1*1](http://img.blog.csdn.net/20171212145859683?)

## 2.6 谷歌 Inception 网络简介

之前我们介绍的CNN单层的滤波算子filter尺寸是固定的，1x1或者3x3等。而Inception Network在单层网络上可以使用多个不同尺寸的filters，进行same convolutions，把各filter下得到的输出拼接起来。除此之外，还可以将CONV layer与POOL layer混合，同时实现各种效果。但是要注意使用same pool。

![Inception](http://img.blog.csdn.net/20171212151353599?)

Inception Network由Christian Szegedy, Wei Liu等人提出。与其它只选择单一尺寸和功能的filter不同，Inception Network使用不同尺寸的filters并将CONV和POOL混合起来，将所有功能输出组合拼接，再由神经网络本身去学习参数并选择最好的模块。

Inception Network在提升性能的同时，会带来计算量大的问题。例如下面这个例子：

![Inception](http://img.blog.csdn.net/20171212172342457?)

此CONV layer需要的计算量为：$28\times 28\times 32\times 5\times 5\times 192 = 120m$，其中m表示百万单位。可以看出但这一层的计算量都是很大的。为此，我们可以引入1x1 Convolutions来减少其计算量，结构如下图所示：

![Inception](http://img.blog.csdn.net/20171212175549666?)

通常我们把该1x1 Convolution称为**瓶颈层（bottleneck layer）**：通道数目先缩小再增大（192->16->32）。引入bottleneck layer之后，总共需要的计算量为：$28\times 28\times 16\times 192 + 28\times 28\times 32\times 5\times 5\times 16 = 12.4m$。明显地，虽然多引入了1x1 Convolution层，但是总共的计算量减少了近90%，效果还是非常明显的。由此可见，1x1 Convolutions还可以有效减少CONV layer的计算量。

## 2.7 Inception 网络

上一节我们使用1x1 Convolution来减少Inception Network计算量大的问题。引入1x1 Convolution后的Inception module如下图所示：

![Inception](http://img.blog.csdn.net/20171213204922094?)

多个Inception modules组成Inception Network，效果如下图所示：

![Inception网络](http://img.blog.csdn.net/20171213211346970?)

上述Inception Network除了由许多Inception modules组成之外，值得一提的是网络中间隐藏层也可以作为输出层Softmax，有利于防止发生过拟合。

## 2.10 数据扩充

常用的数据扩充（Data Augmentation）方法是对已有的样本集进行**镜像（Mirroring）和随机剪裁（Random Cropping）**。

![数据扩充](http://img.blog.csdn.net/20171214102716526?)

另一种Data Augmentation的方法是**色彩变换（color shifting）**。color shifting就是对图片的RGB通道数值进行随意增加或者减少，改变图片色调。

![color shifting](http://img.blog.csdn.net/20171214104357412?)

除了随意改变RGB通道数值外，还可以更有针对性地对图片的RGB通道进行PCA color augmentation，也就是对图片颜色进行主成分分析，对主要的通道颜色进行增加或减少，可以采用高斯扰动做法。这样也能增加有效的样本数量。具体的PCA color augmentation做法可以查阅AlexNet的相关论文。

最后提一下，**在构建大型神经网络的时候，data augmentation和training可以由两个不同的线程来进行**。

## 2.11 计算机视觉现状

神经网络需要数据，不同的网络模型所需的数据量是不同的。Object dection，Image recognition，Speech recognition所需的数据量依次增加。一般来说，如果data较少，那么就需要更多的手工工程（hand-engineering），对已有data进行处理，比如上一节介绍的data augmentation。模型算法也会相对要复杂一些。如果data很多，可以构建深层神经网络，不需要太多的手工工程，模型算法也就相对简单一些。

![CV现状](http://img.blog.csdn.net/20171214113426096?)

值得一提的是手工工程是一项非常重要也比较困难的工作。很多时候，手工工程对模型训练效果影响很大，特别是在数据量不多的情况下。

在模型研究或者竞赛方面，有一些方法能够有助于提升神经网络模型的性能：

- 组合：单独训练几个网络，并平均输出它们的输出。
- Multi-crop 测试：在多个版本的测试图像运行分类器然后取平均结果。

10-crop示例：镜像+中间/上下左右=2*5=10-crop

![10-crop](http://img.blog.csdn.net/20171214123526329?)

但是由于这两种方法计算成本较大，一般仅用于基准测试和赢得竞赛，实际生产系统不用。

最后，我们还要灵活使用开源代码：

- 使用文献中所发表的网络架构
- 如果可能的话，使用开源实现。
- 使用预先训练的模型，并对数据集进行微调。