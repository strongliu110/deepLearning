# 机器学习（ML）策略（1）

## 1.1 为什么是 ML 策略

当我们最初得到一个深度神经网络模型时，我们可能希望从很多方面来对它进行优化，例如：

- 收集更多数据
- 收集更多样的训练集
- 使用梯度下降时，训练更长时间
- 尝试用 Adam 代替梯度下降
- 尝试更大/更小的神经网络
- 尝试 Dropout regularization
- 增加 L2 regularization
- 调整神经网络架构：替换激活函数，增减隐层单元数，……

可选择的方法很多，也很复杂、繁琐。盲目选择、尝试不仅耗费时间而且可能收效甚微。因此，使用快速、有效的策略来优化机器学习模型是非常必要的。

## 1.2 正交化

通过每次只调试一个参数，保持其它参数不变，而得到的模型某一性能改变是一种最常用的调参策略，我们称之为**正交化方法（Orthogonalization）**。

以机器学习中的链式假设为例进行说明：

1. 第一条优化训练集，可以通过使用更复杂NN，使用Adam等优化算法来实现；

2. 第二条优化验证集，可以通过正则化，采用更多训练样本来实现；

3. 第三条优化测试集，可以通过使用更多的验证集样本来实现；

4. 第四条提升实际应用模型，可以通过更换验证集，使用新的cost function来实现。

概括来说，每一种“功能”对应不同的调节方法。而这些调节方法只会对应一个“功能”，是正交的。

正交化的核心在于每次调试一个参数只会影响模型的某一个性能。即在一个阶段使用的技术手段，不会影响另一个阶段。这样，我们就能独立地改进模型在不停阶段的性能，而不必担心对其他阶段造成太大影响。

**早停（early stopping）在模型功能调试中并不推荐使用**。因为早停在提升验证集性能的同时降低了训练集的性能。也就是说早停同时影响两个“功能”，不具有独立性、正交性。

## 1.3 单一数字评估指标

构建、优化机器学习模型时，单值评价指标非常必要。有了量化的单值评价指标后，我们就能根据这一指标比较不同超参数对应的模型的优劣，从而选择最优的那个模型。

举个例子，比如有A和B两个模型，它们的准确率（Precision）和召回率（Recall）分别如下：

![模型](http://img.blog.csdn.net/20171113160716628?)

如果只看Precision的话，B模型更好。如果只看Recall的话，A模型更好。实际应用中，我们通常使用单值评价指标F1 Score来评价模型的好坏。F1 Score综合了Precision和Recall的大小，计算方法如下：

$F1=\frac{2\cdot P\cdot R}{P+R}$

然后得到了A和B模型各自的F1 Score：

![F1 Score](http://img.blog.csdn.net/20171113161842574?)

从F1 Score来看，A模型比B模型更好一些。通过引入**单值评价指标F1 Score**，很方便对不同模型进行比较。

除了F1 Score之外，我们还可以使用**平均值作为单值评价指标**来对模型进行评估。

## 1.4 满足和优化指标

**不同的应用场景，优先考虑的评估指标有所不同**。比如疾病检查，患者关心的是得病的几率，他在乎的是查准率； 但比如疾病预防，医院希望将某疾病的病患都找出来进行隔离治疗，这时医院可能优先考虑查全率，宁抓错，不放过。

有时候，要把所有的性能指标都综合在一起，构成单值评价指标是比较困难的。解决办法是，**可以把某些性能作为优化指标（Optimizing metic），寻求最优化值；而某些性能作为满意指标（Satisficing metic），只要满足阈值就行了**。

## 1.5 训练/开发/测试集划分

**训练集与 (开发集，测试集) 的数据分布可以不同，但开发集与测试集的数据分布必须相同**。

这就好比我们在dev sets上找到最接近一个靶的靶心的箭，但是我们test sets提供的靶心却远远偏离dev sets上的靶心，结果这支肯定无法射中test sets上的靶心位置。

## 1.6 开发集和测试集的大小

对于dev sets数量的设置，应该遵循的准则是：通过dev sets能够检测不同算法或模型的区别，以便选择出更好的模型。

对于test sets数量的设置，应该遵循的准则是：通过test sets能够反映出模型在实际中的表现，即检验最终模型的泛化能力。

实际应用中，可能只有train/dev sets，而没有test sets。这种情况也是允许的，只要算法模型没有对dev sets过拟合。但是，条件允许的话，最好是有test sets，实现无偏估计。

## 1.7 什么时候该改变开发/测试集和指标

举个猫类识别的例子。初始的评价标准是错误率，算法A错误率为3%，算法B错误率为5%。显然，A更好一些。但是，实际使用时发现算法A会通过一些色情图片，但是B没有出现这种情况。**从用户的角度来说，他们可能更倾向选择B模型，虽然B的错误率高一些**。这时候，我们就需要改变之前单纯只是使用错误率作为评价标准，而考虑新的情况进行改变。例如增加色情（porn）图片的权重，增加其代价。

原来的cost function：
$$
J=\frac{1}{m}\sum_{i=1}^{m}{L\left( \hat{y}^{\left( i \right)},y^{\left( i \right)} \right)}
$$
更改评价标准后的cost function：
$$
J=\frac{1}{w^{\left( i \right)}}\sum_{i=1}^{m}{w^{\left( i \right)}L\left( \hat{y}^{\left( i \right)},y^{\left( i \right)} \right)}
$$

$$
w^{(i)}=\begin{cases} 		1, & x^{(i)}\ is\ non-porn\\ 		10, & x^{(i)}\ is\ porn 	\end{cases}
$$

概括来说，机器学习可分为两个过程：设定目标；训练模型向目标看齐。也就是说，第一步是找靶心，第二步是通过训练，射中靶心。但是在训练的过程中可能会根据实际情况改变算法模型的评价标准，进行动态调整。

**另外一个需要动态改变评价标准的情况是dev/test sets与实际使用的样本分布不一致**。比如猫类识别样本图像分辨率差异。

## 1.8 为什么是人的表现

机器学习模型的表现通常会跟人类水平表现作比较，如下图所示：

![模型表现](http://img.blog.csdn.net/20171115090646865?)

图中，横坐标是训练时间，纵坐标是准确性。机器学习模型经过训练会不断接近human-level performance甚至超过它。但是，超过human-level performance之后，准确性会上升得比较缓慢，最终不断接近理想的最优情况，我们称之为bayes optimal error。理论上任何模型都不能超过它，bayes optimal error代表了最佳表现。

人类在某些方面有不俗的表现，例如图像识别、语音识别等领域，人类是很擅长的。当机器学习的水平逊于人类时，要提升其性能，可以参考以下几点：

- 获得人类标记的数据

- 手动分析错误，思考人是如何判断的

- 分析 bias/ variance

## 1.9 可避免偏差 

通常，我们把训练误差（training error）与人类水平误差（human-level error）之间的差值称为bias，也称作**可避免误差（avoidable bias）**；把训练误差与开发误差（dev error）之间的差值称为方差（variance）。根据bias和variance值的相对大小，可以知道算法模型是否发生了欠拟合或者过拟合。

实际应用中，我们一般会用human-level error代表bayes optimal error。

## 1.10 理解人的表现

不同人，对于相同任务的水平是不一样的：专业的和业余的，水平就明显不一样；一群专业的，水平一般也会比一个人高. 根据上面那句准则，人类的最高水平就代表了 Human-level。当然，具体情况还得具体分析。

## 1.11 超过人的表现

对于自然感知类问题，例如视觉、听觉等，机器学习的表现不及人类。但是在很多其它方面，机器学习模型的表现已经超过人类了，包括：在线广告、产品推荐、物流（预测运输时间）、借贷分析。

实际上，机器学习模型超过人类水平是比较困难的。但是只要提供足够多的样本数据，训练复杂的神经网络，模型预测准确性会大大提高，很有可能接近甚至超过人类水平。**值得一提的是当算法模型的表现超过人类水平时，很难再通过人的直觉来解决如何继续提高算法模型性能的问题**。

## 1.12 改善你的模型的表现

提高机器学习模型性能主要要解决两个问题：avoidable bias和variance。我们之前介绍过，training error与human-level error之间的差值反映的是avoidable bias，dev error与training error之间的差值反映的是variance。

**减小可避免误差的常用方法：**

- 训练更大的模型

- 训练更长时间/使用更优的优化算法

- 调整神经网络架构/超参数搜索

 **减小方差的常用方法:**
- 更多的数据
- 正则化 (L2, dropout, 数据增强)
- 调整神经网络架构/超参数搜索