# 机器学习（ML）策略（2）

[TOC]

## 2.1 进行误差分析

对已经建立的机器学习模型进行错误分析（error analysis）十分必要，而且有针对性地、正确地进行error analysis更加重要。

举个例子，猫类识别问题，已经建立的模型的错误率为10%。为了提高正确率，我们发现该模型会将一些狗类图片错误分类成猫。一种常规解决办法是扩大狗类样本，增强模型对狗类（负样本）的训练。但是，这一过程可能会花费几个月的时间，耗费这么大的时间成本到底是否值得呢？也就是说扩大狗类样本，重新训练模型，对提高模型准确率到底有多大作用？这时候我们就需要进行error analysis，帮助我们做出判断。

方法很简单，我们可以从分类错误的样本中统计出狗类的样本数量。根据狗类样本所占的比重，判断这一问题的重要性。假如狗类样本所占比重仅为5%，即时我们花费几个月的时间扩大狗类样本，提升模型对其识别率，改进后的模型错误率最多只会降低到9.5%。相比之前的10%，并没有显著改善。我们把这种性能限制称为**性能天花板（ceiling on performance）**。相反，假如错误样本中狗类所占比重为50%，那么改进后的模型错误率有望降低到5%，性能改善很大。因此，值得去花费更多的时间扩大狗类样本。

error analysis可以同时评估多个影响模型性能的因素，通过各自在错误样本中所占的比例来判断其重要性（**表格分析法**）。

通常来说，比例越大，影响越大，越应该花费时间和精力着重解决这一问题。这种error analysis让我们改进模型更加有针对性，从而提高效率。

## 2.2 清除标记错误的数据

监督式学习中，训练样本有时候会出现输出y标注错误的情况，即incorrectly labeled examples。如果这些label标错的情况是随机性的（random errors），DL算法对其包容性是比较强的，即健壮性好，一般可以直接忽略，无需修复。然而，如果是系统错误（systematic errors），这将对DL算法造成影响，降低模型性能。

刚才说的是训练样本中出现incorrectly labeled data，如果是dev/test sets中出现incorrectly labeled data，该怎么办呢？

方法很简单，利用上节内容介绍的error analysis，**统计dev sets中所有分类错误的样本中incorrectly labeled data所占的比例**。根据该比例的大小，决定是否需要修正所有incorrectly labeled data，还是可以忽略。

我们知道，dev set的主要作用是在不同算法之间进行比较，选择错误率最小的算法模型。但是，如果有incorrectly labeled data的存在，**当不同算法错误率比较接近的时候**，我们无法仅仅根据Overall dev set error准确指出哪个算法模型更好，必须修正incorrectly labeled data。

关于修正incorrect dev/test set data，有几条建议：

- 要处理误标记的问题，应同时处理验证集与测试集的误标记样本，确保同分布。不仅误标记处理，任何对验证集的处理，都应以相同的方式再处理测试集。

- 必要时候，可以对正确分类的数据也进行分析 (也许存在错错得对的情况也说不定)。

- 训练集和验证集/测试集的数据可能来自略微不同的分布。

## 2.3 快速搭建你的第一个系统，并进行迭代

对于如何构建一个机器学习应用模型，Andrew给出的建议：先快速构建第一个简单模型，然后再反复迭代优化。

## 2.4 在不同的划分上进行训练并测试

当训练集与 (验证/测试集) 具有不同的数据分布时，混合所有样本随机洗牌划分数据集是错误的处理方式。**正确的处理方式是，从同分步的验证/测试集中取出部分样本加入训练集**。这样保证了验证集最接近实际应用场合，这种方法较为常用，而且性能表现比较好。很多时候，海量的训练数据是容易获得的，而验证测试数据是稀缺的。

## 2.5 不匹配数据划分的偏差和方差

在可能伴有train set与dev/test set分布不一致的情况下，定位是否出现variance的方法是**设置train-dev set**：从原来的train set中分割出一部分作为train-dev set，train-dev set不作为训练模型使用，而是与dev set一样用于验证。

这样，我们就有training error、training-dev error和dev error三种error。其中，training error与training-dev error的差值反映了variance；training-dev error与dev error的差值反映了data mismatch problem，即样本分布不一致。

**总结一下human-level error、training error、training-dev error、dev error以及test error之间的差值关系和反映的问题**：

![误差关系](http://img.blog.csdn.net/20171123102507240?)

可避免误差（Avoidable bias），方差（Variance），样本分布不一致（Data mismatch），验证集的过拟合程度（degree of overfitting to dev set） 。

一般情况下，human-level error、training error、training-dev error、dev error以及test error的数值是递增的，但是也会出现dev error和test error下降的情况。这主要可能是因为训练样本比验证/测试样本更加复杂，难以训练。

## 2.6 解决数据不匹配问题

关于如何解决train set与dev/test set样本分布不一致的问题，有两条建议：

- 手动分析误差，理解训练集与验证/测试集的差异。
- 使训练数据更接近验证/测试数据；或者收集更多与验证/测试数据相似的数据加入训练集。

为了让train set与dev/test set类似，我们可以使用**人工数据合成的方法（artificial data synthesis）**。例如说话人识别问题，实际应用场合（dev/test set）是包含背景噪声的，而训练样本train set很可能没有背景噪声。为了让train set与dev/test set分布一致，我们可以在train set上人工添加背景噪声，合成类似实际场景的声音。这样会让模型训练的效果更准确。

**人工数据合成需要注意的地方**：我们不能给每段语音都增加同一段背景噪声，这样会出现对背景噪音的过拟合，效果不佳。

## 2.7 迁移学习

深度学习非常强大的一个功能之一就是有时候你可以将已经训练好的模型的一部分知识（网络结构）直接应用到另一个类似模型中去。比如我们已经训练好一个猫类识别的神经网络模型，那么我们可以直接把该模型中的一部分网络结构应用到使用X光片预测疾病的模型中去。这种学习方法被称为**迁移学习（Transfer Learning）**。

如果我们已经有一个训练好的神经网络，用来做图像识别。现在，我们想要构建另外一个通过X光片进行诊断的模型。迁移学习的做法是无需重新构建新的模型，而是利用之前的神经网络模型，只改变样本输入、输出以及输出层的权重系数$W^{\left[ L \right]}$，$b^{\left[ L \right]}$。也就是说对新的样本(X,Y)，重新训练输出层权重系数$W^{\left[ L \right]}$，$b^{\left[ L \right]}$，而其它层所有的权重系数$W^{\left[ L \right]}$，$b^{\left[ L \right]}$保持不变。**理论依据是，图片识别与放射诊断的低层特征是相似的，比如线条的特征，这就是两个学习任务共同的知识。**

![迁移学习](http://img.blog.csdn.net/20171123160012172?)

顺便提一下，如果重新训练所有权重系数，初始$W^{\left[ L \right]}$，$b^{\left[ L \right]}$由之前的模型训练得到，这一过程称为pre-training。之后，不断调试、优化$W^{\left[ L \right]}$，$b^{\left[ L \right]}$的过程称为fine-tuning。pre-training和fine-tuning分别对应上图中的黑色箭头和红色箭头。

迁移学习可以保留原神经网络的一部分，再添加新的网络层。具体问题，具体分析，可以去掉输出层后再增加额外一些神经层。

**总结一下迁移学习的一些前提条件:**

- 任务 A，B 具有相同的输入。学习任务是相近的，知识是相通的。
- 要将任务 A 习得的知识迁移到任务 B，要求任务 A 的数据量大于 B。否则知识不可靠且没有太大意义。
- A 的低层特征对学习任务 B 有帮助。

## 2.8 多任务学习

顾名思义，**多任务学习（multi-task learning）**就是构建神经网络同时执行多个任务。这跟二元分类或者多元分类都不同，多任务学习类似将多个神经网络融合在一起，用一个网络模型来实现多种分类效果。如果有C个，那么输出y的维度是(C,1)。

值得一提的是，Multi-task learning与Softmax regression的区别在于Softmax regression是single label的，即输出向量y只有一个元素为1；而Multi-task learning是multiple labels的，即输出向量y可以有多个元素为1。

多任务学习是使用单个神经网络模型来实现多个任务。实际上，也可以分别构建多个神经网络来实现。但是，如果各个任务之间是相似问题（例如都是图片类别检测），则可以使用多任务学习模型。另外，多任务学习中，可能存在训练样本Y某些label空白的情况，这并不影响多任务模型的训练。

**总结一下多任务学习的一些前提条件:**

- 学习的一系列任务，都能从共享的低层特征中收益。比如图片中多物体的识别，低层特征如线条就是共享的，所有的物体识别都能从中获益。此处有一个推论，就是得益于特征共享，多任务学习的性能会比独立训练多个单任务神经网络更好。
- 通常，各学习任务的数据量是相近的。以自动驾驶为例，要学习识别行人，车辆，交通标志，信号灯，那么，能够识别出行人，车辆，交通标志，信号灯的样本数量应该是相近的。一个直观的感觉是，有 1000000 个样本，其中 999000 能识别出车辆，只有 1000 样本能识别出行人，那么行人不是更像噪声吗。
- 只要训练一个足够大的神经网络，就能在各项任务上都有足够好的性能。

顺便提一下，**迁移学习和多任务学习在实际应用中，迁移学习使用得更多一些**。

## 2.9 什么是端到端的深度学习

**端到端（end-to-end）深度学习**就是将所有不同阶段的数据处理系统或学习系统模块组合在一起，用一个单一的神经网络模型来实现所有的功能。它将所有模块混合在一起，只关心输入和输出。

以语音识别为例，传统的算法流程和end-to-end模型的区别如下：

![端到端学习](http://img.blog.csdn.net/20171127134439086?)

如果训练样本足够大，神经网络模型足够复杂，那么end-to-end模型性能比传统机器学习分块模型更好。实际上，end-to-end让神经网络模型内部去自我训练模型特征，自我调节，增加了模型整体契合度。

## 2.10 是否要使用端到端的深度学习

端到端学习的优缺点:

- 优点:
  - 让数据发声
  - 只需要更少的人工参与
- 缺点:
  - 可能需要大量的数据，大到足以让数据发声
  - 剔除了潜在的有帮助的人工参与

人工参与到机器学习/深度学习的各环节，可以将人的经验，知识等注入系统，也许能帮助系统更好更快的学习。此外，将端到端学习拆分成多个相对独立的学习过程，有点分治的味道，易于实现，性能也可能比设计不佳的端到端学习更好。