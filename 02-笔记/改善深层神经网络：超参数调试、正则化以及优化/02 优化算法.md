## 优化算法

## 2.1 Mini-batch 梯度下降法

之前我们介绍的神经网络训练过程是对所有m个样本，称为batch，通过向量化计算方式，同时进行的。如果m很大，例如达到百万数量级，训练速度往往会很慢，因为每次迭代都要对所有样本进行进行求和运算和矩阵运算。我们将这种梯度下降算法称为**批量梯度下降（Batch Gradient Descent）**。

为了解决这一问题，我们可以把m个训练样本分成若干个子集，称为mini-batches，这样每个子集包含的数据量就小了，例如只有1000，然后每次在单一子集上进行神经网络训练，速度就会大大提高。这种梯度下降算法叫做**小批量梯度下降（Mini-batch Gradient Descent）**。

假设总的训练样本个数m=5000000，其维度为($n_{x}$,m)。将其分成5000个子集，每个mini-batch含有1000个样本。我们将每个mini-batch记为$X^{\left\{ t \right\}}$，其维度为($n_{x}$,1000)。相应的每个mini-batch的输出记为$Y^{\left\{ t \right\}}$，其维度为(1,1000)，且$t=1,2,\dotsc,5000$。

这里顺便总结一下我们遇到的神经网络中几类字母的上标含义：

$X^{\left( i \right)}$：第i个样本

$Z^{\left( l \right)}$：神经网络第l层网络的线性输出

$X^{\left\{ t \right\}}$,$Y^{\left\{ t \right\}}$：第t组mini-batch

**Mini-batches Gradient Descent的实现过程**是先将总的训练样本分成T个子集（mini-batches），然后对每个mini-batch进行神经网络训练，包括Forward Propagation，Compute Cost Function，Backward Propagation，循环至T个mini-batch都训练完毕。

```python
for  t=1,⋯,T  {
	Forward Propagation
	ComputeCostFunction
	BackwardPropagation
	W:=W−α⋅dW
	b:=b−α⋅db
}
```

经过T次循环之后，所有m个训练样本都进行了梯度下降计算。这个过程，我们称之为经历了一个epoch。对于Batch Gradient Descent而言，一个epoch只进行一次梯度下降算法；而Mini-Batches Gradient Descent，一个epoch会进行T次梯度下降算法。

值得一提的是，对于Mini-Batches Gradient Descent，可以进行多次epoch训练。而且，**每次epoch，最好是将总体训练数据重新打乱、重新分成T组mini-batches，这样有利于训练出最佳的神经网络模型**。

## 2.2 理解 Mini-batch 梯度下降法

Batch gradient descent和Mini-batch gradient descent的cost曲线如下图所示：

![梯度下降](http://img.blog.csdn.net/20171026113219156?)

对于一般的神经网络模型，使用Batch gradient descent，随着迭代次数增加，cost是不断减小的。然而，使用Mini-batch gradient descent，随着在不同的mini-batch上迭代训练，其cost不是单调下降，而是受类似noise的影响，出现振荡。但整体的趋势是下降的，最终也能得到较低的cost值。

**之所以出现细微振荡的原因是不同的mini-batch之间是有差异的**。例如可能第一个子集$\left( X^{\left\{ 1 \right\}},Y^{\left\{ 1 \right\}} \right)$是好的子集，而第二个子集$\left( X^{\left\{ 2 \right\}},Y^{\left\{ 2 \right\}} \right)$包含了一些噪声noise。出现细微振荡是正常的。

如何选择每个mini-batch的大小，即包含的样本个数呢？有两个极端：如果mini-batch size=m，即为批量梯度下降（Batch gradient descent），只包含一个子集为$\left( X^{\left\{ 1 \right\}},Y^{\left\{ 1 \right\}} \right)=\left( X,Y \right)$；如果mini-batch size=1，即为**随机梯度下降（Stachastic gradient descent）**，每个样本就是一个子集$\left( X^{\left\{ 1 \right\}},Y^{\left\{ 1 \right\}} \right)=\left( x^{\left( i \right)},y^{\left( i \right)} \right)$，共有m个子集。

BGD会比较平稳地接近全局最小值，但是因为使用了所有m个样本，每次前进的速度有些慢。SGD每次前进速度很快，但是路线曲折，有较大的振荡，最终会在最小值附近来回波动，难以真正达到最小值处。而且在数值处理上就不能使用向量化的方法来提高运算速度。

实际使用中，mini-batch size不能设置得太大（Batch gradient descent），也不能设置得太小（Stachastic gradient descent）。这样，相当于结合了Batch gradient descent和Stachastic gradient descent各自的优点，**既能使用向量化优化算法，又能叫快速地找到最小值**。

一般来说，如果总体样本数量m不太大时，例如m≤2000，建议直接使用Batch gradient descent。如果总体样本数量m很大时，建议将样本分成许多mini-batches。推荐常用的mini-batch size为64,128,256,512。这些都是2的幂。**之所以这样设置的原因是计算机存储数据一般是2的幂，这样设置可以提高运算速度**。

## 2.3 指数加权平均



