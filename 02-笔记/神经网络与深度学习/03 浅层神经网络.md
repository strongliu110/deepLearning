# 浅层神经网络

## 3.1 神经网络概览

神经网络的结构与逻辑回归类似，只是神经网络的层数比逻辑回归多一层，多出来的中间那层称为隐藏层或中间层。这样从计算上来说，神经网络的正向传播和反向传播过程只是比逻辑回归多了一次重复的计算。正向传播过程分成两层，第一层是输入层到隐藏层，用上标[1]来表示：

$z^{\left[ 1 \right]}=W^{\left[ 1 \right]}x+b^{\left[ 1 \right]}$

$a^{\left[ 1 \right]}=\sigma \left( z^{\left[ 1 \right]} \right)$

第二层是隐藏层到输出层，用上标[2]来表示：

$z^{\left[ 2 \right]}=W^{\left[ 2 \right]}a^{\left[ 1 \right]}+b^{\left[ 2 \right]}$

$a^{\left[ 2 \right]}=\sigma \left( z^{\left[ 2 \right]} \right)$

在写法上值得注意的是，方括号上标[i]表示当前所处的层数；圆括号上标(i)表示第i个样本。

同样，反向传播过程也分成两层。第一层是输出层到隐藏层，第二层是隐藏层到输入层。

## 3.2 神经网络表示

单隐藏层神经网络就是典型的浅层（shallow）神经网络。

![单隐藏层神经网络](http://img.blog.csdn.net/20170927081305727?)

在写法上，我们通常把输入矩阵X记为$a^{\left[ 0 \right]}$，把隐藏层输出记为$a^{\left[ 1 \right]}$，上标从0开始。用下标表示第几个神经元，注意下标从1开始。例如$a_{1}^{\left[ 1 \right]}$表示隐藏层第1个神经元，$a_{2}^{\left[ 1 \right]}$表示隐藏层第2个神经元，等等。这样，隐藏层有4个神经元就可以将其输出$a^{\left[ 1 \right]}$写成矩阵的形式：
$$
a^{\left[ 1 \right]}=\left[ \begin{array}{c} a_{1}^{\left[ 1 \right]} \\ a_{2}^{\left[ 1 \right]} \\ a_{3}^{\left[ 1 \right]} \\ a_{4}^{\left[ 1 \right]} \end{array} \right]
$$
最后，相应的输出层记为$a^{\left[ 2 \right]}$，即ŷ 。

通常我们只会计算隐藏层输出和输出层的输出，输入层是不用计算的。这也是我们把输入层层数上标记为0的原因（$a^{\left[ 0 \right]}$）。

关于隐藏层对应的权重$w^{\left[ 1 \right]}$和常数项$b^{\left[ 1 \right]}$，$w^{\left[ 1 \right]}$的维度是（4,3）。这里的4对应着隐藏层神经元个数，3对应着输入层x特征向量包含元素个数。常数项$b^{\left[ 1 \right]}$的维度是（4,1）。关于输出层对应的权重$w^{\left[ 2 \right]}$和常数项$b^{\left[ 2 \right]}$，$w^{\left[ 2 \right]}$的维度是（1,4），这里的1对应着输出层神经元个数，4对应着输出层神经元个数。常数项b[2]的维度是（1,1），因为输出只有一个神经元。

总结一下，**第i层的权重$w^{\left[ i \right]}$维度的行等于i层神经元的个数，列等于i-1层神经元的个数；第i层常数项$b^{\left[ i \right]}$维度的行等于i层神经元的个数，列始终为1**。

## 3.3 计算神经网络的输出

![计算神经网络的输出](http://img.blog.csdn.net/20170927081331442?)

**一个神经元可以看作是两步计算的结合: 1. 输入的线性加权$z=w^{T}x+b$； 2. 非线性变换$a=\sigma \left( z \right)$**

对于两层神经网络，从输入层到隐藏层对应一次逻辑回归运算；从隐藏层到输出层对应一次逻辑回归运算。每层计算时，要注意对应的上标和下标，一般我们记上标方括号表示layer，下标表示第几个神经元。例如a[l]i表示第l层的第i个神经元。注意，i从1开始，l从0开始。

为了提高程序运算速度，我们引入向量化和矩阵运算的思想，将上述表达式转换成矩阵运算的形式：

![这里写图片描述](http://img.blog.csdn.net/20170927081521144?)

其中，$W^{\left[ 1 \right]}$的维度是（4,3），$b^{\left[ 1 \right]}$的维度是（4,1），$W^{\left[ 2 \right]}$的维度是（1,4），$b^{\left[ 2 \right]}$的维度是（1,1）。

## 3.4 多个例子中的向量化

在书写标记上用上标(i)表示第i个样本。

对于每个样本i，可以使用for循环来求解其正向输出：

for i = 1 to m: 
​	$z^{\left[ 1 \right]\left( i \right)}=w^{\left[ 1 \right]}x^{\left( i \right)}+b^{\left[ 1 \right]}$

​	$a^{\left[ 1 \right]\left( i \right)}=\sigma \left( z^{\left[ 1 \right]\left( i \right)} \right)$

​	$z^{\left[ 2 \right]\left( i \right)}=w^{\left[ 2 \right]}a^{\left[ 1 \right]\left( i \right)}+b^{\left[ 2 \right]}$

​	$a^{\left[ 2 \right]\left( i \right)}=\sigma \left( z^{\left[ 2 \right]\left( i \right)} \right)$

不使用for循环，利用矩阵运算的思想，输入矩阵X的维度为（$n^{x}$,m）。这样，我们可以把上面的for循环写成矩阵运算的形式：

$Z^{\left[ 1 \right]}=W^{\left[ 1 \right]}X+b^{\left[ 1 \right]}$

$A^{\left[ 1 \right]}=\sigma \left( Z^{\left[ 1 \right]} \right)$

$Z^{\left[ 2 \right]}=W^{\left[ 2 \right]}A^{\left[ 1 \right]}+b^{\left[ 1 \right]}$

$A^{\left[ 2 \right]}=\sigma \left( Z^{\left[ 2 \right]} \right)$

其中，$Z^{\left[ 1 \right]}$的维度是（4,m），4是隐藏层神经元的个数；$A^{\left[ 1 \right]}$的维度与$Z^{\left[ 1 \right]}$相同；$Z^{\left[ 2 \right]}$和$A^{\left[ 2 \right]}$的维度均为（1,m）。对上面这四个矩阵来说，均可以这样来理解：**行表示神经元个数，列表示样本数目m**。

## 3.5 向量化实现的解释

值得注意的是输入矩阵X也可以写成$A^{\left[ 0 \right]}$。

## 3.6 激活函数

![sigmoid](http://img.blog.csdn.net/20170920120759651?)

![tanh](http://img.blog.csdn.net/20170919091551404?)

![relu](http://img.blog.csdn.net/20170919091933651?)

![leaky relu](http://img.blog.csdn.net/20170919092253605?)

如何选择合适的激活函数呢？

首先我们来比较sigmoid函数和tanh函数。对于隐藏层的激活函数，一般来说，tanh函数要比sigmoid函数表现更好一些。因为tanh函数的取值范围在[-1,+1]之间，隐藏层的输出被限定在[-1,+1]之间，可以看成是在0值附近分布，均值为0。这样从隐藏层到输出层，数据起到了归一化（均值为0）的效果。因此，隐藏层的激活函数，tanh比sigmoid更好一些。而对于输出层的激活函数，因为二分类问题的输出取值为{0,+1}，所以一般会选择sigmoid作为激活函数。

观察sigmoid函数和tanh函数，我们发现有这样一个问题，就是当|z|很大的时候，激活函数的斜率（梯度）很小。因此，在这个区域内，梯度下降算法会运行得比较慢。在实际应用中，应尽量避免使z落在这个区域，使|z|尽可能限定在零值附近，从而提高梯度下降算法运算速度。

为了弥补sigmoid函数和tanh函数的这个缺陷，就出现了ReLU激活函数。ReLU激活函数在z大于零时梯度始终为1；在z小于零时梯度始终为0；z等于零时的梯度可以当成1也可以当成0，实际应用中并不影响。对于隐藏层，选择ReLU作为激活函数能够保证z大于零时梯度始终为1，从而提高神经网络梯度下降算法运算速度。但当z小于零时，存在梯度为0的缺点，实际应用中，这个缺点影响不是很大。为了弥补这个缺点，出现了Leaky ReLU激活函数，能够保证z小于零是梯度不为0。

最后总结一下，如果是分类问题，输出层的激活函数一般会选择sigmoid函数。但是隐藏层的激活函数通常不会选择sigmoid函数，tanh函数的表现几乎在所用场合都会比sigmoid函数好一些。**实际应用中，通常会会选择使用ReLU或者Leaky ReLU函数，保证梯度下降速度不会太小**。其实，具体选择哪个函数作为激活函数没有一个固定的准确的答案，应该要根据具体实际问题进行验证。

## 3.7 为什么需要非线性激活函数

假设所有的激活函数都是线性的，为了简化计算，我们直接令激活函数g(z)=z，即a=z。那么，浅层神经网络的各层输出为：

$z^{\left[ 1 \right]}=W^{\left[ 1 \right]}x+b^{\left[ 1 \right]}$

$a^{\left[ 1 \right]}=z^{\left[ 1 \right]}$

$z^{\left[ 2 \right]}=W^{\left[ 2 \right]}a^{\left[ 1 \right]}+b^{\left[ 2 \right]}$

$a^{\left[ 2 \right]}=z^{\left[ 2 \right]}$

我们对上式中$a^{\left[ 2 \right]}$进行化简计算：

$a^{\left[ 2 \right]}=W^{\left[ 2 \right]}\left( W^{\left[ 1 \right]}x+b^{\left[ 1 \right]} \right)+b^{\left[ 2 \right]}=W^{\left[ 2 \right]}W^{\left[ 1 \right]}x+W^{\left[ 2 \right]}b^{\left[ 1 \right]}+b^{\left[ 2 \right]}=W'x+b$

经过推导我们发现$a^{\left[ 2 \right]}$仍是输入变量x的线性组合。这表明，使用神经网络与直接使用线性模型的效果并没有什么两样。

值得一提的是，**如果是预测问题而不是分类问题，输出y是连续的情况下，输出层的激活函数可以使用线性函数**。如果输出y恒为正值，则也可以使用ReLU激活函数。

## 3.8 激活函数的导数

sigmoid函数的导数：
$$
g\left( z \right)=\frac{1}{1+e^{-z}}
$$

$$
g'\left( z \right)=g\left( z \right)\left( 1-g\left( z \right) \right)
$$

tanh函数的导数：
$$
g\left( z \right)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}
$$

$$
g'\left( z \right)=1-g^{2}\left( z \right)
$$

ReLU函数的导数：
$$
g\left( z \right)=\max \left( 0,z \right)
$$

$$
g'(z)=
\begin{cases}
0 & {z<0}\\
1 & {z \geq 0}
\end{cases}
$$

Leaky ReLU函数的导数：
$$
g\left( z \right)=\max \left( 0.01z,z \right)
$$

$$
g'(z)=
\begin{cases}
0.01 & {z<0}\\
1 & {z \geq 0}
\end{cases}
$$

## 3.9 神经网络的梯度下降法

