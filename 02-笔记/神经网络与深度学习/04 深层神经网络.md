# 深层神经网络

[TOC]

## 4.1 深层神经网络

深层神经网络其实就是包含更多的隐藏层神经网络。

## 4.2 深层网络中的前向传播

对于第l层，其正向传播过程的$Z^{\left[ l \right]}$和$A^{\left[ l \right]}$可以表示为：

$Z^{\left[ l \right]}=W^{\left[ l \right]}A^{\left[ l-1 \right]}+b^{\left[ l \right]}$

$A^{\left[ l \right]}=g^{\left[ l \right]}\left( Z^{\left[ l \right]} \right)$

其中$l=1,\dotsc,L$

## 4.3 核对矩阵的维数

**确保深度神经网络正常运行的一个方法是: 检查矩阵的维度。**

对于m个训练样本，输入矩阵X的维度是（$n^{\left[ 0 \right]}$,m）。需要注意的是$W^{\left[ l \right]}$和$b^{\left[ l \right]}$的维度与只有单个样本是一致的：

$W^{\left[ l \right]}:\left( n^{\left[ l \right]},n^{\left[ l-1 \right]} \right)$

$b^{\left[ l \right]}:\left( n^{\left[ l \right]},1 \right)$

只不过在运算$Z^{\left[ l \right]}=W^{\left[ l \right]}A^{\left[ l-1 \right]}+b^{\left[ l \right]}$中，$b^{\left[ l \right]}$会被当成（$n^{\left[ l \right]}$,m）矩阵进行运算，这是因为python的广播性质，且$b^{\left[ l \right]}$每一列向量都是一样的。$dW^{\left[ l \right]}$和$db^{\left[ l \right]}$的维度分别与$W^{\left[ l \right]}$和$b^{\left[ l \right]}$的相同。

但是，$Z^{\left[ l \right]}$和$A^{\left[ l \right]}$的维度发生了变化：

$Z^{\left[ l \right]}:\left( n^{\left[ l \right]},m \right)$

$A^{\left[ l \right]}:\left( n^{\left[ l \right]},m \right)$

$dZ^{\left[ l \right]}$和$dA^{\left[ l \right]}$的维度分别与$Z^{\left[ l \right]}$和$A^{\left[ l \right]}$的相同。

## 4.4 为什么使用深层表示

如果隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高。神经网络从左到右，神经元提取的特征从简单到复杂。特征复杂度与神经网络层数成正相关。特征越来越复杂，功能也越来越强大。

**除了从提取特征复杂度的角度来说明深层网络的优势之外，深层网络还有另外一个优点，就是能够减少神经元个数，从而减少计算量**。

深度神经网络更关注隐层的数量, 而不是神经元的数量. L 层的小型深度神经网络可能比拥有大量隐层单元的浅度网络, 拥有更好的性能。

如下图, 要计算 n 个特征的异或：$y=x_{1} \oplus x_{2} \oplus x_{3} \oplus \dotsb \oplus x_{n}$ , 深度神经网络时间复杂度为$O\left( \log \; n \right)$, 而单隐层神经网络, 时间复杂度为 $O\left( 2^{n} \right)$。比较下来，处理同一逻辑问题，深层网络所需的神经元个数比浅层网络要少很多。

![/img/2017-08-21-andrew_ng_dl_course1_note/circuit-theory_deep-learning.png](http://kissg.me/img/2017-08-21-andrew_ng_dl_course1_note/circuit-theory_deep-learning.png)

尽管深度学习有着非常显著的优势，但是对实际问题进行建模时，尽量先选择层数少的神经网络模型，这也符合**奥卡姆剃刀定律**（Occam’s Razor）。对于比较复杂的问题，再使用较深的神经网络模型。

## 4.5 搭建深层神经网络块

对于第l层来说，正向传播过程中：

输入：$a^{\left[ l-1 \right]}$

输出：$a^{\left[ l \right]}$

参数：$W^{\left[ l \right]}$，$b^{\left[ l \right]}$

缓存变量：$z^{\left[ l \right]}$

反向传播过程中：

输入：$da^{\left[ l \right]}$

输出：$da^{\left[ l-1 \right]}$，$dW^{\left[ l \right]}$，$db^{\left[ l \right]}$

参数：$W^{\left[ l \right]}$，$b^{\left[ l \right]}$

![/img/2017-08-21-andrew_ng_dl_course1_note/forwardprop-backprop_kiank.png](http://kissg.me/img/2017-08-21-andrew_ng_dl_course1_note/forwardprop-backprop_kiank.png)



对于神经网络所有层，整体的流程块图正向传播过程和反向传播过程如下所示：

![搭建深层神经网络](http://img.blog.csdn.net/20171019095613497?)

## 4.6 前向和反向传播

首先是正向传播过程，令层数为第l层，输入是$a^{\left[ l-1 \right]}$，输出是$a^{\left[ l \right]}$，缓存变量是$z^{\left[ l \right]}$。其表达式如下：

$z^{\left[ l \right]}=W^{\left[ l \right]}a^{\left[ l-1 \right]}+b^{\left[ l \right]}$

$a^{\left[ l \right]}=g^{\left[ l \right]}\left( z^{\left[ l \right]} \right)$

m个训练样本，向量化形式为：

$Z^{\left[ l \right]}=W^{\left[ l \right]}A^{\left[ l-1 \right]}+b^{\left[ l \right]}$

$A^{\left[ l \right]}=g^{\left[ l \right]}\left( Z^{\left[ l \right]} \right)$

然后是反向传播过程，输入是$da^{\left[ l \right]}$，输出是：$da^{\left[ l-1 \right]}$，$dW^{\left[ l \right]}$，$db^{\left[ l \right]}$。其表达式如下：

$dz^{\left[ l \right]}=da^{\left[ l \right]}\times g^{'\left[ l \right]}\left( z^{\left[ l \right]} \right)$

$dW^{\left[ l \right]}=dz^{\left[ l \right]}\cdot a^{\left[ l-1 \right]}$

$db^{\left[ l \right]}=dz^{\left[ l \right]}$

$da^{\left[ l-1 \right]}=W^{\left[ l \right]T}\cdot dz^{\left[ l \right]}$

由上述第四个表达式可得$da^{\left[ l \right]}=W^{\left[ l+1 \right]T}\cdot dz^{\left[ l+1 \right]}$，将$da^{\left[ l \right]}$代入第一个表达式中可以得到：

$dz^{\left[ l \right]}=W^{\left[ l+1 \right]T}\cdot dz^{\left[ l+1 \right]}\times g'^{\left[ l \right]}\left( z^{\left[ l \right]} \right)$

**该式非常重要，反映了$dz^{\left[ l-1 \right]}$与$dz^{\left[ l \right]}$的递推关系**。

m个训练样本，向量化形式为：

$dZ^{\left[ l \right]}=dA^{\left[ l \right]}\times g^{'\left[ l \right]}\left( Z^{\left[ l \right]} \right)$

$dW^{\left[ l \right]}=\frac{1}{m}dZ^{\left[ l \right]}\cdot A^{\left[ l-1 \right]T}$

$db^{\left[ l \right]}=\frac{1}{m}np.sum\left( dZ^{\left[ l \right]},axis=1,keepdim=True \right)$

$dA^{\left[ l-1 \right]}=W^{\left[ l \right]T}\cdot dZ^{\left[ l \right]}$

$dZ^{\left[ l \right]}=W^{\left[ l+1 \right]T}\cdot dZ^{\left[ l+1 \right]}\times g'^{\left[ l \right]}\left( Z^{\left[ l \right]} \right)$

## 4.7 参数 VS 超参数

神经网络中的参数就是我们熟悉的$W^{\left[ l \right]}$和$b^{\left[ l \right]}$。而超参数则是例如学习速率$\alpha $，训练迭代次数N，神经网络层数L，各层神经元个数$n^{\left[ l \right]}$，激活函数g(z)等。**之所以叫做超参数的原因是它们决定了参数$W^{\left[ l \right]}$和$b^{\left[ l \right]}$的值**。

## 4.8 这和大脑有什么关系？

神经网络实际上可以分成两个部分：正向传播过程和反向传播过程。神经网络的每个神经元采用激活函数的方式，类似于感知机模型。这种模型与人脑神经元是类似的，可以说是一种非常简化的人脑神经元模型。如下图所示，人脑神经元可分为树突、细胞体、轴突三部分。树突接收外界电刺激信号（类比神经网络中神经元输入），传递给细胞体进行处理（类比神经网络中神经元激活函数运算），最后由轴突传递给下一个神经元（类比神经网络中神经元输出）。

------

机器学习/深度学习的基本流程:

![/img/2017-08-21-andrew_ng_dl_course1_note/workflow_of_nn.png](http://kissg.me/img/2017-08-21-andrew_ng_dl_course1_note/workflow_of_nn.png)

